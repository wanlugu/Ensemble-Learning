# Ensemble-Learning

https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/

## Bagging Ensemble Learning
## Stacking Ensemble Learning
## Boosting Ensemble Learning
XGBoost hyperparameter tuning guide https://www.kaggle.com/code/prashant111/a-guide-on-xgboost-hyperparameters-tuning/notebook. \
Bayesian optimization guide https://www.kaggle.com/code/prashant111/bayesian-optimization-using-hyperopt/notebook\
- The idea behind Bayesian optimization is instead of defining a fixed search space, more time is spent in deciding which values to try next. 
- Parameters are the configuration model, which are internal to the model, it is changed during model training. Hyperparameters are the explicitly specified parameters that control the training process and will stay unchanged. Parameters are essential for making predictions. Hyperparameters are essential for optimizing the model.
